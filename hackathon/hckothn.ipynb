{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11509022,"sourceType":"datasetVersion","datasetId":7216441}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain_groq langchain_community faiss-cpu python-pptx PyMuPDF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:02:06.594816Z","iopub.execute_input":"2025-04-22T10:02:06.595094Z","iopub.status.idle":"2025-04-22T10:02:11.809145Z","shell.execute_reply.started":"2025-04-22T10:02:06.595072Z","shell.execute_reply":"2025-04-22T10:02:11.808396Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import os\nimport re\nimport io\nimport json\nfrom typing import List, Dict\nimport numpy as np\nfrom PIL import Image\nfrom pydub import AudioSegment, effects\nimport fitz \nimport torch\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForVision2Seq,\n    AutoModelForSpeechSeq2Seq,\n    pipeline,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough, RunnableParallel\n\nfrom langchain_community.vectorstores import FAISS\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_groq import ChatGroq\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pydantic import BaseModel\nfrom torch.nn.attention import SDPBackend, sdpa_kernel ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:02:13.303229Z","iopub.execute_input":"2025-04-22T10:02:13.303492Z","iopub.status.idle":"2025-04-22T10:02:13.422439Z","shell.execute_reply.started":"2025-04-22T10:02:13.303470Z","shell.execute_reply":"2025-04-22T10:02:13.421778Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class WhisperTranscriptionAgent:\n    def __init__(self, model_id=\"openai/whisper-large-v3\"):\n        torch.set_float32_matmul_precision(\"high\")\n        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\n        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            model_id, torch_dtype=self.torch_dtype, low_cpu_mem_usage=True\n        ).to(self.device)\n\n        self.model.generation_config.cache_implementation = \"static\"\n        self.model.generation_config.max_new_tokens = 256\n        self.model.forward = torch.compile(self.model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=self.model,\n            tokenizer=self.processor.tokenizer,\n            feature_extractor=self.processor.feature_extractor,\n            chunk_length_s=120,\n            batch_size=4,\n            torch_dtype=self.torch_dtype,\n            device=self.device,\n        )\n\n    def extract_audio(self, video_path: str, output_audio_path: str = \"temp_audio.wav\") -> str:\n        audio = AudioSegment.from_file(video_path)\n        audio.export(output_audio_path, format=\"wav\")\n        return output_audio_path\n\n    def preprocess_audio(self, audio_path: str) -> np.ndarray:\n        audio = AudioSegment.from_file(audio_path)\n        audio = effects.normalize(audio)\n        audio = audio.set_frame_rate(16000).set_channels(1)\n        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2 ** 15)\n        return samples\n\n    def transcribe(self, video_path: str, output_txt: str = \"transcript.txt\") -> str:\n        audio_path = self.extract_audio(video_path)\n        samples = self.preprocess_audio(audio_path)\n\n        with sdpa_kernel(SDPBackend.MATH):\n            result = self.pipe(samples, generate_kwargs={\"language\": \"en\"})\n\n        with open(output_txt, \"w\") as f:\n            f.write(result[\"text\"])\n\n        os.remove(audio_path)  \n        return result[\"text\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:26:58.246625Z","iopub.execute_input":"2025-04-22T09:26:58.247244Z","iopub.status.idle":"2025-04-22T09:26:58.255363Z","shell.execute_reply.started":"2025-04-22T09:26:58.247221Z","shell.execute_reply":"2025-04-22T09:26:58.254716Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    agent = WhisperTranscriptionAgent()\n    video_file = \"/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\"\n    transcript = agent.transcribe(video_file)\n    print(\"Transcription Complete:\\n\", transcript)\n    with open(\"transcript.txt\", \"w\") as f:\n        f.write(transcript)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:56:26.345700Z","iopub.execute_input":"2025-04-22T09:56:26.346265Z","iopub.status.idle":"2025-04-22T10:01:38.730422Z","shell.execute_reply.started":"2025-04-22T09:56:26.346239Z","shell.execute_reply":"2025-04-22T10:01:38.729638Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Transcription Complete:\n  . . . . I you are going to get in return weights right. Normally all these are based on neural network all the models that we have talked about are all based on neural networks and hence we are going to get the models learnings in the form of one layer but all of the layers that are present in your transformer. If they are all transformer based layers. So several layers, several heads all of these together consists of these weights. Is this clear? That is what we mean by the weight matrix. So as an full precision for those numbers. And by now all of you know that the numbers always underlying computer are represented in the form of binary. There is nothing representation and this is used for every number. Do you understand? 176 billion is the bits into 32 bits into 32 bits. 176 billion parameters crossing 1600 GB if you are storing all of that. In memory I am talking about RAM. That is why only OpenAI, Google, Microsoft, these well known meta, all of these only can afford to do this. I was and I am given to understand that they are going to learn and use them. During inference we need to have those weights in your memory for you to run. If you want to have the model in to let us say 400 GB, let us say some company can afford to have So, if I extend it to 3 bits 8 numbers can be represented. So, more the number of bits the range that you can have is higher is not it. So, same thing is true here this is for number in a 16 bit precision, the number degrades to 3.125. Is that clear? The binary bits that you can accommodate here, when you convert back to decimal, you will only get 3.125. So, there is an error here. So, the moment you start representing a floating point number in 16 bits, the amount of information that it communic parameter that you will give for the training arguments. When you give that then it will accommodate in a smaller memory area when compared to the 32 bit precision. So this is important. And based on that the number of floating point operations would also be different. So it will come down. So flops also will have an impact on how many bits you are going to extract only certain regions in this. You want to quantize, you want to pick this, you want to pick this, you want to pick this, you want to pick this. So, only certain discrete values you will pick up. So, if the signal is close to this, suppose your signal measurement is close to this, you will still assign this number. So, you will only take certain quantization steps. Depending on infinity to plus infinity. But you are going to quantize it to only certain set of discrete values. And use those discrete values for your further processing. representation. So, generally followed is a 4 bit quantization, 4 bit quantization. It may go in some sequence but I am just writing use only these ranges, nothing else. And each of these ranges represent an integer number. And how many ranges have taken 16 levels which is nothing but 4 bit quantization. If there are 16 levels it is called as 4 bit quantization because to represent these numbers I just need 4 bits. If there are 16 numbers you need 4 bits to represent. So, I will still take a smaller number here for just for reference because I am just using minus 1 to 1 little bit higher also you can take. So, 1.8 that is also still closer to this number. So, 2 or 1.8 does not matter it is close to 1 it is close to 1. So, what is the number corresponding to 1? 15 how many different types of integers are there depends on how many quantization levels you will have. All of the floating point numbers they in the form of a quantized representation. This is the quantization step. So for further downstream task, I will still use floating point 16 or 32 only. But I will use that on the fly. When I want just about to retrieve one block of weights, I will pick up, de-quantize it and use the floating point number only for further processing. Do billion parameters, I am successful in storing it on my system if I have one RTX 4090 with 80 GB or one A100, one A100 GPU with 80 GB. That much of possibility is there. But does it mean that I am going to continue to use these numbers for my further processing? These are integer numbers. They will not make sense. They are not the learned weights actually. The learned weights are something else. I in LLM is fine tuning. Fine tuning there are plenty of methods that are available. But in our course here we are restricting to one type called as SPEFT. Parameter is fine tuning? Now I have discussed so much of Harikate about pre-trained models, weights are there, 176 billion parameters. The entire Harikate I have told you. Now you tell me what is fine learning. So 176 billion parameters model you will load. Last but one bench Bhavana and your friend, please be silent. 176 billion parameters you will load and you will give your new data set and all the 176 billion parameters get an update. All 176 billion parameters get If you have taken a 176 billion parameter GPT model, updating all 176 billion parameters based on the new data set. This is known as full fine tuning. Full fine tuning. It is parameter efficient fine tuning. This you should always remember. LoRa is under PEFT. Parameter efficient fine tuning. So, let us say your original stored, 25 weights 5 cross 5. What LoRa does is, now I have a new data set, let us say PES data set I have. I want to train the model. Now, additionally you have to create two more layers which are initialized with weights, the two layers are initialized with weights and the two layers are split up, they are decomposed based on the weights, based on the rank of the matrix model along with the adapter? The adapter layers contain two matrices kind of weights 5 cross 1 and 1 cross 5 in this example updated. Now the two low rank matrices are used for some purpose. I will tell you that. and then you add them. But there is another variation. There is alpha by r. Alpha by r. R stands for some information, A has some information. When you, it is possible to decompose a rectangular matrix, otherwise PCI is only for square matrix, but SVD can be used for rectangular matrix and you can do the decomposition. But if you observe the singular value matrix, it is a diagonal matrix. You you can do is you can cut off actually this portion. You can remove some singular values. So, when you do the matrix multiplication, it will automatically reduce the dimensions that they will learn first and then they will blow it up to a new matrix by combining the two that is used in low rank matrices? Instead of injecting a 5 cross 5 like full blown up type of matrix, the LoRa used the linear algebra. You just understand this much right now. LoRa used the linear algebra and produce. But for inference time you will still need it. 25 size you need inference time. But that is for a short period. Sometimes what they do is they don't do the full thing. I will tell you that also. But is the concept clear? Now how much input comes. What you should do? The new input comes and you have to give an answer to that new input X. This is the inference time. Test image you are giving or test line you are giving. At that time you need to perform this operation on the fly. B into A, 25. Scale the lower of H and then add it to original. That is the final total learning.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from pathlib import Path\nfrom pptx import Presentation\n\ndef extract_text_from_shape(shape):\n    content = []\n    if shape.has_text_frame:\n        for paragraph in shape.text_frame.paragraphs:\n            level = paragraph.level\n            text = paragraph.text.strip()\n            if text:\n                content.append((\"  \" * level) + text)\n    return content\n\ndef parse_pptx_file(pptx_path: str) -> str:\n    pptx_path = Path(pptx_path)\n    prs = Presentation(pptx_path)\n    text_lines = []\n\n    for idx, slide in enumerate(prs.slides):\n        text_lines.append(f\"[PPTX] {pptx_path.name} - Slide {idx + 1}\")\n        title = slide.shapes.title.text.strip() if slide.shapes.title else \"No Title\"\n        text_lines.append(f\"Title: {title}\")\n\n        for shape in slide.shapes:\n            if shape == slide.shapes.title:\n                continue\n            if shape.has_text_frame:\n                lines = extract_text_from_shape(shape)\n                text_lines.extend(lines)\n\n        if slide.has_notes_slide and slide.notes_slide.notes_text_frame:\n            notes_text = slide.notes_slide.notes_text_frame.text.strip()\n            if notes_text:\n                text_lines.append(f\"Notes: {notes_text}\")\n\n    return \"\\n\".join(text_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:42:17.294374Z","iopub.execute_input":"2025-04-22T09:42:17.294662Z","iopub.status.idle":"2025-04-22T09:42:17.524428Z","shell.execute_reply.started":"2025-04-22T09:42:17.294637Z","shell.execute_reply":"2025-04-22T09:42:17.523676Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"parsed_text = parse_pptx_file(\"/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/Class6_Unit3_Trees_ThreadBST.pptx\")\n\nwith open(\"parsed_slide_text.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(parsed_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:43:43.254587Z","iopub.execute_input":"2025-04-22T09:43:43.255195Z","iopub.status.idle":"2025-04-22T09:43:43.585010Z","shell.execute_reply.started":"2025-04-22T09:43:43.255172Z","shell.execute_reply":"2025-04-22T09:43:43.584453Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class PDFExtractorAgent:\n    def __init__(self, output_text_dir=\"/kaggle/working/extracted_pdf_content/text\",\n                 output_image_dir=\"/kaggle/working/extracted_pdf_content/images\"):\n        self.output_text_dir = output_text_dir\n        self.output_image_dir = output_image_dir\n        os.makedirs(self.output_text_dir, exist_ok=True)\n        os.makedirs(self.output_image_dir, exist_ok=True)\n\n    def sanitize_filename(self, name):\n        return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n\n    def filter_text_content(self, text):\n        filtered_lines = []\n        skip_section = False\n\n        lines = text.split('\\n')\n        email_pattern = re.compile(r'\\S+@\\S+\\.\\S+')\n        course_code_pattern = re.compile(r'[A-Z]{2}\\d{2}[A-Z]{2}\\d{3}[A-Z]{2}\\d?')\n        name_title_pattern = re.compile(r'^(Dr\\.|Prof\\.|Mr\\.|Mrs\\.|Ms\\.) ')\n        acknowledgment_pattern = re.compile(r'^(Ack|Acknowledgment|Acknowledgement)', re.IGNORECASE)\n\n        for i, line in enumerate(lines):\n            if not line.strip():\n                continue\n            if email_pattern.search(line) or course_code_pattern.search(line):\n                continue\n            if name_title_pattern.match(line.strip()):\n                continue\n            if acknowledgment_pattern.match(line.strip()):\n                skip_section = True\n                continue\n            if skip_section:\n                if line.strip().isupper() or (i < len(lines) - 1 and not lines[i + 1].strip()):\n                    skip_section = False\n                else:\n                    continue\n            if \"Department of\" in line or \"University\" in line or \"Centre for\" in line:\n                continue\n            if len(line.strip()) < 10 and not line.strip().isdigit():\n                continue\n            filtered_lines.append(line)\n\n        return '\\n'.join(filtered_lines)\n\n    def extract(self, pdf_path: str) -> bool:\n        \"\"\"Extracts filtered text and images from a PDF and saves results to disk.\"\"\"\n        subfolder = os.path.basename(os.path.dirname(pdf_path))\n        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n\n        try:\n            doc = fitz.open(pdf_path)\n            all_text = []\n\n            for page_num, page in enumerate(doc):\n                text = page.get_text()\n                if text.strip():\n                    filtered = self.filter_text_content(text)\n                    if filtered.strip():\n                        all_text.append(filtered)\n\n                for img_index, img_info in enumerate(page.get_images(full=True)):\n                    xref = img_info[0]\n                    base_image = doc.extract_image(xref)\n                    image_bytes = base_image[\"image\"]\n\n                    image_ext = base_image[\"ext\"]\n                    image_filename = f\"{subfolder}-{pdf_name}_page{page_num+1}_image{img_index+1}.{image_ext}\"\n                    image_path = os.path.join(self.output_image_dir, self.sanitize_filename(image_filename))\n\n                    with open(image_path, \"wb\") as image_file:\n                        image_file.write(image_bytes)\n                    print(f\"Image saved: {image_filename}\")\n\n            if all_text:\n                text_filename = f\"{subfolder}-{pdf_name}.txt\"\n                text_path = os.path.join(self.output_text_dir, self.sanitize_filename(text_filename))\n                with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n                    text_file.write('\\n\\n'.join(all_text))\n                print(f\"Text saved: {text_filename}\")\n\n            doc.close()\n            return True\n\n        except Exception as e:\n            print(f\"[ERROR] Failed to extract from {pdf_path}: {e}\")\n            return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:02:18.759584Z","iopub.execute_input":"2025-04-22T10:02:18.759882Z","iopub.status.idle":"2025-04-22T10:02:18.777777Z","shell.execute_reply.started":"2025-04-22T10:02:18.759861Z","shell.execute_reply":"2025-04-22T10:02:18.776873Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"pdfagent = PDFExtractorAgent()\npdfagent.extract(\"/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/Finetuning.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:02:23.543684Z","iopub.execute_input":"2025-04-22T10:02:23.543980Z","iopub.status.idle":"2025-04-22T10:02:38.330252Z","shell.execute_reply.started":"2025-04-22T10:02:23.543957Z","shell.execute_reply":"2025-04-22T10:02:38.329515Z"}},"outputs":[{"name":"stdout","text":"Image saved: Lora_Qlora-Finetuning_page1_image1.png\nImage saved: Lora_Qlora-Finetuning_page2_image1.png\nImage saved: Lora_Qlora-Finetuning_page2_image2.png\nImage saved: Lora_Qlora-Finetuning_page3_image1.png\nImage saved: Lora_Qlora-Finetuning_page4_image1.png\nImage saved: Lora_Qlora-Finetuning_page4_image2.png\nImage saved: Lora_Qlora-Finetuning_page5_image1.png\nImage saved: Lora_Qlora-Finetuning_page5_image2.png\nImage saved: Lora_Qlora-Finetuning_page6_image1.png\nImage saved: Lora_Qlora-Finetuning_page6_image2.png\nImage saved: Lora_Qlora-Finetuning_page7_image1.png\nImage saved: Lora_Qlora-Finetuning_page7_image2.png\nImage saved: Lora_Qlora-Finetuning_page8_image1.png\nImage saved: Lora_Qlora-Finetuning_page8_image2.png\nImage saved: Lora_Qlora-Finetuning_page9_image1.png\nImage saved: Lora_Qlora-Finetuning_page10_image1.png\nImage saved: Lora_Qlora-Finetuning_page11_image1.png\nImage saved: Lora_Qlora-Finetuning_page12_image1.png\nImage saved: Lora_Qlora-Finetuning_page13_image1.png\nImage saved: Lora_Qlora-Finetuning_page13_image2.png\nImage saved: Lora_Qlora-Finetuning_page14_image1.png\nImage saved: Lora_Qlora-Finetuning_page14_image2.png\nImage saved: Lora_Qlora-Finetuning_page15_image1.png\nImage saved: Lora_Qlora-Finetuning_page15_image2.png\nImage saved: Lora_Qlora-Finetuning_page16_image1.png\nImage saved: Lora_Qlora-Finetuning_page17_image1.png\nImage saved: Lora_Qlora-Finetuning_page18_image1.png\nImage saved: Lora_Qlora-Finetuning_page19_image1.png\nImage saved: Lora_Qlora-Finetuning_page20_image1.png\nImage saved: Lora_Qlora-Finetuning_page20_image2.png\nImage saved: Lora_Qlora-Finetuning_page21_image1.png\nImage saved: Lora_Qlora-Finetuning_page22_image1.png\nImage saved: Lora_Qlora-Finetuning_page23_image1.png\nText saved: Lora_Qlora-Finetuning.txt\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"class ImageDescriptionAgent:\n    def __init__(self, model_name=\"microsoft/git-large-coco\"):\n        print(\"Loading image captioning model...\")\n        self.model_name = model_name\n        self.processor = AutoProcessor.from_pretrained(model_name)\n        self.model = AutoModelForVision2Seq.from_pretrained(model_name)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n    def describe_image(self, image_path):\n        \"\"\"\n        Generate a description for a single image.\n        \"\"\"\n        try:\n            image = Image.open(image_path).convert('RGB')\n            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n            \n            generated_ids = self.model.generate(\n                pixel_values=inputs.pixel_values,\n                max_length=100,\n                num_beams=3,\n                early_stopping=True\n            )\n            \n            description = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            return description\n        except Exception as e:\n            print(f\"[ERROR] Failed to process {image_path}: {e}\")\n            return \"Uncaptionable image\"\n\n    def describe_images_in_folder(self, folder_path, save_to):\n        all_files = [f for f in os.listdir(folder_path) ]\n\n        results = {}\n        for filename in all_files:\n            path = os.path.join(folder_path, filename)\n            print(f\"Describing: {filename}\")\n            description = self.describe_image(path)\n            if \"pes\" in description or \"logo\" in description or \"PES\" in description:\n                continue\n            results[filename] = description\n\n        if save_to:\n            with open(save_to, \"w\", encoding=\"utf-8\") as f:\n                json.dump(results, f, indent=2)\n            print(f\"Saved results to {save_to}\")\n\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:09:33.025014Z","iopub.execute_input":"2025-04-22T10:09:33.025634Z","iopub.status.idle":"2025-04-22T10:09:33.033350Z","shell.execute_reply.started":"2025-04-22T10:09:33.025606Z","shell.execute_reply":"2025-04-22T10:09:33.032773Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"image = ImageDescriptionAgent()\nimage.describe_images_in_folder(\"/kaggle/working/extracted_pdf_content/images\", \"desc.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:09:33.247668Z","iopub.execute_input":"2025-04-22T10:09:33.247842Z","iopub.status.idle":"2025-04-22T10:10:54.779960Z","shell.execute_reply.started":"2025-04-22T10:09:33.247829Z","shell.execute_reply":"2025-04-22T10:10:54.779194Z"}},"outputs":[{"name":"stdout","text":"Loading image captioning model...\nDescribing: Lora_Qlora-Finetuning_page17_image1.png\nDescribing: Lora_Qlora-Finetuning_page7_image1.png\nDescribing: Lora_Qlora-Finetuning_page13_image2.png\nDescribing: Lora_Qlora-Finetuning_page13_image1.png\nDescribing: Lora_Qlora-Finetuning_page16_image1.png\nDescribing: Lora_Qlora-Finetuning_page15_image2.png\nDescribing: Lora_Qlora-Finetuning_page4_image2.png\nDescribing: Lora_Qlora-Finetuning_page5_image2.png\nDescribing: Lora_Qlora-Finetuning_page22_image1.png\nDescribing: Lora_Qlora-Finetuning_page20_image2.png\nDescribing: Lora_Qlora-Finetuning_page14_image1.png\nDescribing: Lora_Qlora-Finetuning_page15_image1.png\nDescribing: Lora_Qlora-Finetuning_page6_image2.png\nDescribing: Lora_Qlora-Finetuning_page19_image1.png\nDescribing: Lora_Qlora-Finetuning_page8_image2.png\nDescribing: Lora_Qlora-Finetuning_page2_image1.png\nDescribing: Lora_Qlora-Finetuning_page2_image2.png\nDescribing: Lora_Qlora-Finetuning_page11_image1.png\nDescribing: Lora_Qlora-Finetuning_page12_image1.png\nDescribing: Lora_Qlora-Finetuning_page6_image1.png\nDescribing: Lora_Qlora-Finetuning_page23_image1.png\nDescribing: Lora_Qlora-Finetuning_page10_image1.png\nDescribing: Lora_Qlora-Finetuning_page21_image1.png\nDescribing: Lora_Qlora-Finetuning_page20_image1.png\nDescribing: Lora_Qlora-Finetuning_page4_image1.png\nDescribing: Lora_Qlora-Finetuning_page7_image2.png\nDescribing: Lora_Qlora-Finetuning_page14_image2.png\nDescribing: Lora_Qlora-Finetuning_page18_image1.png\nDescribing: Lora_Qlora-Finetuning_page3_image1.png\nDescribing: Lora_Qlora-Finetuning_page9_image1.png\nDescribing: Lora_Qlora-Finetuning_page1_image1.png\nDescribing: Lora_Qlora-Finetuning_page5_image1.png\nDescribing: Lora_Qlora-Finetuning_page8_image1.png\nSaved results to desc.json\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'Lora_Qlora-Finetuning_page13_image2.png': 'a schematic diagram of the algorithm.',\n 'Lora_Qlora-Finetuning_page15_image2.png': 'a screenshot of a cell phone description generated with very high confidence',\n 'Lora_Qlora-Finetuning_page4_image2.png': 'a screenshot of a cell phone description automatically generated',\n 'Lora_Qlora-Finetuning_page5_image2.png': 'a diagram of the process.',\n 'Lora_Qlora-Finetuning_page20_image2.png': 'a diagram of the system.',\n 'Lora_Qlora-Finetuning_page6_image2.png': 'a screenshot of a cell phone description automatically generated',\n 'Lora_Qlora-Finetuning_page8_image2.png': 'a screenshot of a cell phone description automatically generated',\n 'Lora_Qlora-Finetuning_page2_image2.png': 'a diagram of a document with a piece of paper connected to it',\n 'Lora_Qlora-Finetuning_page7_image2.png': 'a diagram showing how to make a braille forget.',\n 'Lora_Qlora-Finetuning_page14_image2.png': 'a diagram of a neural network.'}"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"class AgentConfig(BaseModel):\n    groq_api_key: str\n    model_name: str = \"mistral-saba-24b\"\n    transcript_path: str = \"transcript.txt\"\n    slide_path: str = \"/kaggle/working/extracted_pdf_content/text/Lora_Qlora-Finetuning.txt\"\n    image_descriptions_path: str = \"/kaggle/working/desc.json\"\n    output_path: str = \"lecture_notes.md\"\n    chunk_size: int = 1000\n    chunk_overlap: int = 200\n    temperature: float = 0.3\n    max_tokens: int = 4000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:33:41.146444Z","iopub.execute_input":"2025-04-22T10:33:41.147057Z","iopub.status.idle":"2025-04-22T10:33:41.152439Z","shell.execute_reply.started":"2025-04-22T10:33:41.147033Z","shell.execute_reply":"2025-04-22T10:33:41.151857Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class LectureNotesAgent:\n    def __init__(self, config: AgentConfig):\n        self.config = config\n        self.llm = ChatGroq(\n            api_key=config.groq_api_key,\n            model_name=config.model_name,\n            temperature=config.temperature,\n            max_tokens=config.max_tokens\n        )\n        self.embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=config.chunk_size,\n            chunk_overlap=config.chunk_overlap,\n            length_function=len,\n            is_separator_regex=False,\n        )\n        self.few_shot_prompt = self._few_shot_examples()\n        self.hierarchical_prompt = self._get_hierarchical_prompt()\n        self.summary_prompt = ChatPromptTemplate.from_template(self._summary_template())\n        self.connection_prompt = ChatPromptTemplate.from_template(self._connection_template())\n\n    def load_data(self) -> Dict[str, str]:\n        with open(self.config.transcript_path, 'r') as f:\n            transcript = f.read()\n        with open(self.config.slide_path, 'r') as f:\n            slides = f.read()\n    \n        # Load image descriptions\n        with open(self.config.image_descriptions_path, 'r') as f:\n            images = json.load(f)\n        image_descriptions = \"\\n\".join(\n            f\"[Image: {img} - {images[img]}]\" for img in images\n        )\n    \n        return {\n            \"transcript\": transcript,\n            \"slides\": slides,\n            \"image_descriptions\": image_descriptions\n        }\n\n    def _create_retriever(self, texts: List[str]) -> MultiQueryRetriever:\n        docs = [Document(page_content=text) for text in texts]\n        vectorstore = FAISS.from_documents(docs, self.embedder)\n        return MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=self.llm)\n\n    def _few_shot_examples(self):\n        examples = [\n            {\n                \"input\": \"Transcript mentions 'neural networks' and slides show 'CNN architecture'\",\n                \"output\": \"1. Neural Networks\\n   1.1 Convolutional Neural Networks (CNNs)\\n      - Specialized for processing grid-like data (images)\\n      - Architecture includes convolutional layers, pooling layers\"\n            },\n            {\n                \"input\": \"Transcript discusses 'backpropagation' and slides mention 'gradient descent'\",\n                \"output\": \"2. Training Neural Networks\\n   2.1 Backpropagation\\n      - Algorithm for calculating gradients\\n      - Used in conjunction with gradient descent\\n   2.2 Gradient Descent\\n      - Optimization algorithm\\n      - Uses gradients from backpropagation\"\n            }\n        ]\n        return FewShotChatMessagePromptTemplate(\n            example_prompt=ChatPromptTemplate.from_messages([\n                (\"human\", \"{input}\"),\n                (\"ai\", \"{output}\")\n            ]),\n            examples=examples\n        )\n\n    def _get_hierarchical_prompt(self):\n        template = \"\"\"\n        You are an expert at creating well-structured educational notes from lecture materials.\n        Combine the transcript, slide content, and relevant image descriptions to create comprehensive notes with the following structure:\n        \n        1. Main Topic\n           1.1 Key Concept\n              - Definition\n              - Examples\n              - Related ideas from other sections\n           1.2 Key Concept\n           1.3 key concept\n            ....\n              ...\n        2. Next Main Topic\n        3. Examples present in the transcript and slides\n           3.1 Example 1\n               - Explain step by step how the result was achieved\n               - If relevant, include a reference to an image and describe what it shows\n           3.2 Example 2\n               - Explain step by step how the result was achieved\n               - If relevant, include a reference to an image and describe what it shows\n        \n        image descripptions are provided with image name and description. based on the key concept header, include the image name in the notes as well\n        [Image: image_name - description]\n        \n        Transcript content:\n        {transcript}\n        \n        Slide content:\n        {slides}\n        \n        Relevant images:\n        {image_descriptions}\n        \n        Generate the structured notes:\n        \"\"\"\n\n        return ChatPromptTemplate.from_messages([\n            (\"system\", \"You are an expert at creating well-structured educational notes.\"),\n            self.few_shot_prompt,\n            (\"human\", template)\n        ])\n\n    def _summary_template(self):\n        return \"\"\"Write a concise summary of the following lecture section, preserving all critical information:\n        {content}\n        Concise summary:\"\"\"\n\n    def _connection_template(self):\n        return \"\"\"Identify connections between these concepts from different parts of the lecture:\n        \n        Concept 1: {concept1}\n        Concept 2: {concept2}\n        \n        Explain how these concepts relate to each other in the context of this lecture:\"\"\"\n\n    def generate_notes(self, data: Dict[str, str]) -> str:\n        transcript_chunks = self.splitter.split_text(data[\"transcript\"])\n        slide_chunks = self.splitter.split_text(data[\"slides\"])\n    \n        transcript_retriever = self._create_retriever(transcript_chunks)\n        slide_retriever = self._create_retriever(slide_chunks)\n    \n        # Set up the retriever chain\n        base_chain = RunnableParallel({\n            \"transcript\": transcript_retriever,\n            \"slides\": slide_retriever,\n        })\n    \n        # Manually merge in static data like image_descriptions\n        full_chain = (\n            base_chain\n            | (lambda inputs: {\n                **inputs,\n                \"image_descriptions\": data[\"image_descriptions\"]\n            })\n            | self.hierarchical_prompt\n            | self.llm\n            | StrOutputParser()\n        )\n    \n        notes = full_chain.invoke({\n            \"transcript\": data[\"transcript\"],\n            \"slides\": data[\"slides\"],\n        })\n    \n        # Append summary\n        summary_chain = self.summary_prompt | self.llm | StrOutputParser()\n        summary = summary_chain.invoke({\"content\": notes})\n        notes += f\"\\n\\n## Overall Summary\\n{summary}\"\n    \n        return notes\n\n\n    def run(self):\n        data = self.load_data()\n        notes = self.generate_notes(data)\n        with open(self.config.output_path, \"w\") as f:\n            f.write(notes)\n        print(f\"Notes saved to {self.config.output_path}\")\n        return notes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:33:41.970387Z","iopub.execute_input":"2025-04-22T10:33:41.971091Z","iopub.status.idle":"2025-04-22T10:33:41.984122Z","shell.execute_reply.started":"2025-04-22T10:33:41.971066Z","shell.execute_reply":"2025-04-22T10:33:41.983471Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    cfg = AgentConfig(\n        groq_api_key=\"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\",\n        transcript_path=\"transcript.txt\",\n        slide_path=\"parsed_slide_text.txt\",\n        image_descriptions_path=\"/kaggle/working/desc.json\",\n        output_path=\"lecture_notes.md\"\n    )\n\n    agent = LectureNotesAgent(cfg)\n    agent.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:59:20.250476Z","iopub.execute_input":"2025-04-22T10:59:20.251202Z","iopub.status.idle":"2025-04-22T10:59:23.153206Z","shell.execute_reply.started":"2025-04-22T10:59:20.251178Z","shell.execute_reply":"2025-04-22T10:59:23.151966Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4273459310.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLectureNotesAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/3425180825.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mnotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3425180825.py\u001b[0m in \u001b[0;36mgenerate_notes\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    145\u001b[0m         )\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         notes = full_chain.invoke({\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;34m\"transcript\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transcript\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;34m\"slides\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"slides\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m         return cast(\n\u001b[1;32m    367\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    936\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 results.append(\n\u001b[0;32m--> 759\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    760\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1003\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         }\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mistral-saba-24b` in organization `org_01jsehbtgjft68bw7vpeg2aw34` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7850, Requested 4504. Please try again in 1m3.544s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"],"ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'message': 'Rate limit reached for model `mistral-saba-24b` in organization `org_01jsehbtgjft68bw7vpeg2aw34` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7850, Requested 4504. Please try again in 1m3.544s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","output_type":"error"}],"execution_count":71},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}